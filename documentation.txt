1. In pytorch there are 2 options to work with data - 
    1.1 DataLoader
    Dataloader represents a python interable over a dataset with support for map style and iterable style datasets.
    - In map style datasets the omputer can learn from a card set with the help of a guide ( class) and randomly access any of the
    cards from the deck.
    - In the iterable style the computer has to follow a sequence in picking up each card like a line ( in an iterative fashion) - 
    Reads are expensive.

    1.2 Dataset
    A Dataset in PyTorch is like a collection of data samples. Each sample typically consists of an input (like an image or some other 
    data) and its corresponding label (the output or the expected result).
    The Dataset class is an abstraction that you can use to represent your data in a way that PyTorch understands.
     It allows you to access individual samples and their labels.
     
     
    In summary, Dataset stores your data samples and labels, and DataLoader helps you manage and iterate through the
    dataset efficiently by providing batches of data during training

2. Every torch vision dataset includes 2 arguments : transform and target transform.
    2.1 Transform : The data that we are working with is image data, which can not be directly fed to a neural network.
    So, the transform argument converts the input image data to a pytorch tensor which can be fed to a neural network as an input.   

3. We pass the 'Dataset' as an argument to the 'DataLoader'.
This wraps an interable over our dataset and supports sampling, shuffling, automatic batching etc. 

4. Creating dataloaers for the train and test data we will find that our input tensor has 4 dimensions
    Batch size , Channels , Height, Width (Since it is an image tensor).

5. In pytorch there is a base class for all neural networks called Module.
    Whatever model you might create must be a subclass of this class -> nn.Module

    - __init__(self) : The constructor of this class will have
        super.__init__() --> which will call the constructor of parent class nn.Module

    - Next step involves the flatten Module
    A neural network expects a 1D array like input but our tensor has 4D representation 
    (batch size, channel , height , width) so flatten module will convert the tensor to required format which the neural network ca understand.    
    but in the constructor the flatten module is just declared - not used.

    - Obvious question is that why is it declared if it is not used right now? Because when different instances of this
    class will be created they will have the flatten module also initialized by default which is quite
    necessary since neural networks only take 1D vector size inputs so - flatten module will always be required.

    - The forward ( ) method : This will take the tensor of the image as an input through its argument. 
    The real flattening operation will take place here on the input tensor x which will then be passed 
    to the neural network stored within the variable ```logits```.

    - In the constructor within nn.Sequqntial where the neural network architecture is defined 
    nn.Linear takes an input 28 * 28 referring that the height and width of the input image is 28*28 pixels and the output should be 512 pixels.

    -- nn.Linear can also be called a fully connected layer which means a layer which connects one neuron to all other neurons in the next layer.

    -- Then each fully connected layer is followed by an activation fuction to introduce non-linearity

    -- Finally, the last linear layer returns the output 10 -> which is the number of classes among which we are supposed to classify the input image .

6. Loss Function
    We have used the CrossEntropyLoss( ) which is commonly used loss fuction in classification tasks.
    specially where there are multiple classes.

    It is often employed in combination with softmax activation and negative log - likelihood loss.

    6.1 Sotmax Activation :
        - It is applied to the raw output logits of the network to convert
        them into class probabilities. This ensures that the output is a probability distribution over the classes.

    6.2 Negative Log Likelihood Loss : 
        - Negative log likelihood loss measures the difference between the
         predicted class probabilities and the true distribution.    

    6.3 Input and target
        - Input to CrossEntropyLoss is the raw output logits from the neural network and the target labels.

        - Target labels should be provided as class indices.  

7.  In a single training loop the model maked predictions on the training dataset ( fed to it in batches)
    and backpropagates the prediction error to adjust the model's parameters.

8. train function : 
    - size captures the # of samples 
    - model.train ( ) specifies the training mode has begun
    - Itrate over batches in the Dataloader
    - Move the data from dataloader to the specified device
    - model(X) computes the predictions of the model on the input data
    - loss_fn(pred,y) calculates loss between pred values and true labels
    - loss.backward( ) --> computes the gradients of the model parameters wrt the loss
    - optimizer.step() updates model parameters based on the computed gradients
    - optimizer.zero_grad() clears gradients to prepare for the next iteration
    - bath % 100 == 0 block prints training loss and current progress every 100 batches
    - batch + 1* len(X) calculates the # of processed samples
This function is responsible for a single iteration of training on a batch of data.
It performs forward and backward passes, updates the model parameters, and prints the training loss and progress.

9. Test function : 
    - size is set to the total # of samples in the test dataset
    - num_batches is the total # of batches in the test dataloader
    - model.eval() sets the model to evaluation mode {This is necessary because models like dropout or batch normalization behave differently during training and evaluation}
    - test_loss and correct are used to accumulate total test loss and count the # of correctly classified samples
    - torch.no_grad() temporarily disables gradient computation during the evaluation to save memory and speed up the process
    - Divide the accumulated test loss by the# of batches to get the average test loss
The test function evaluates the model on the test dataset, providing information about accuracy
and average loss.

10. Analyzing the data
    - torch.randint- > generates a random tensor
    it has the upperbound = length of the training_data
    the size of output tensor is set to 1D
    .item() method extracts single random integer as a python scalar (not as tensor)
    - Retrieve the image and label according to the random index

11. Evaluate a trained and loaded neural network on a single test sample
    - Set model to evaluation mode
    - Retrieve test sample x and its ocrresponding label y from the test dataset
    - Temporarily disable gradient computation since this is inference step
    - move the input data to the device to ensure consistency with the device on which the model is located
    - get prediction
    - argmax(0) finds the index of the maximum value along the specified axis
    This gives index of the class with the highest predicted probability.
    classes[pred[0].argmax(0)] --> gives the corresponding class label

 INFERENCE :
 Refers to the proces sof using a trained model to make predictions based on new or unseen data.
 During inference predictions are generated without updating the parameters
 which makes it differ from the training stage
 Gradient is also not calculated ( obviously - parameter is not updating so what are we going to do with the gradient)


